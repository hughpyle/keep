"""
Pending work queue using SQLite.

Stores work items (summarization, analysis, etc.) for serial background
processing. This enables fast indexing with lazy summarization, and
ensures heavy ML work (MLX model loading) is serialized to prevent
memory exhaustion.

Dequeue is atomic: items transition from 'pending' to 'processing' with
a PID claim inside a single IMMEDIATE transaction. Concurrent processors
cannot grab the same items. Stale claims (crashed processors) are
recovered automatically.

Failed items use exponential backoff before retry (30s, 60s, 120s, ...
up to 1h). Items that exhaust MAX_ATTEMPTS are moved to 'failed' status
(dead letter) rather than deleted, preserving the error for diagnosis.
"""

import json
import logging
import os
import sqlite3
import threading
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional

logger = logging.getLogger(__name__)

# Claims older than this are considered stale (processor crashed)
STALE_CLAIM_SECONDS = 600  # 10 minutes

# Retry backoff: min(BASE * 2^(attempts-1), MAX) seconds
RETRY_BACKOFF_BASE = 30     # 30 seconds initial delay
RETRY_BACKOFF_MAX = 3600    # 1 hour maximum delay


@dataclass
class PendingSummary:
    """A queued work item awaiting processing."""
    id: str
    collection: str
    content: str
    queued_at: str
    attempts: int = 0
    task_type: str = "summarize"
    metadata: dict = field(default_factory=dict)


class PendingSummaryQueue:
    """
    SQLite-backed queue for pending background work.

    Items are added during fast indexing (with truncated placeholder summary)
    or by enqueue_analyze, and processed later by `keep pending`
    or programmatically. All work is serialized to prevent concurrent
    ML model loading.
    """

    def __init__(self, queue_path: Path):
        """
        Args:
            queue_path: Path to SQLite database file
        """
        self._queue_path = queue_path
        self._conn: Optional[sqlite3.Connection] = None
        self._lock = threading.Lock()
        self._init_db()

    def _init_db(self) -> None:
        """Initialize the SQLite database."""
        self._queue_path.parent.mkdir(parents=True, exist_ok=True)
        # isolation_level=None gives us manual transaction control
        # so we can use BEGIN IMMEDIATE for atomic dequeue
        self._conn = sqlite3.connect(
            str(self._queue_path), check_same_thread=False,
            isolation_level=None,
        )

        # Enable WAL mode for better concurrent access across processes
        self._conn.execute("PRAGMA journal_mode=WAL")
        # Wait up to 5 seconds for locks instead of failing immediately
        self._conn.execute("PRAGMA busy_timeout=5000")

        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS pending_summaries (
                id TEXT NOT NULL,
                collection TEXT NOT NULL,
                content TEXT NOT NULL,
                queued_at TEXT NOT NULL,
                attempts INTEGER DEFAULT 0,
                task_type TEXT DEFAULT 'summarize',
                metadata TEXT DEFAULT '{}',
                status TEXT DEFAULT 'pending',
                claimed_by TEXT,
                claimed_at TEXT,
                last_error TEXT,
                retry_after TEXT,
                PRIMARY KEY (id, collection, task_type)
            )
        """)
        self._conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_queued_at
            ON pending_summaries(queued_at)
        """)

        # Migrate existing databases: add new columns if missing
        self._migrate()

        # Index on status (safe after migration ensures column exists)
        self._conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_status
            ON pending_summaries(status)
        """)

    def _migrate(self) -> None:
        """Migrate existing databases to current schema."""
        cursor = self._conn.execute("PRAGMA table_info(pending_summaries)")
        columns = {row[1] for row in cursor.fetchall()}

        if "task_type" not in columns:
            # Old schema: PK is (id, collection). Recreate with (id, collection, task_type).
            # Preserve any pending items during migration.
            self._conn.execute("""
                CREATE TABLE IF NOT EXISTS pending_summaries_new (
                    id TEXT NOT NULL,
                    collection TEXT NOT NULL,
                    content TEXT NOT NULL,
                    queued_at TEXT NOT NULL,
                    attempts INTEGER DEFAULT 0,
                    task_type TEXT DEFAULT 'summarize',
                    metadata TEXT DEFAULT '{}',
                    status TEXT DEFAULT 'pending',
                    claimed_by TEXT,
                    claimed_at TEXT,
                    last_error TEXT,
                    retry_after TEXT,
                    PRIMARY KEY (id, collection, task_type)
                )
            """)
            self._conn.execute("""
                INSERT OR IGNORE INTO pending_summaries_new
                    (id, collection, content, queued_at, attempts)
                SELECT id, collection, content, queued_at, attempts
                FROM pending_summaries
            """)
            self._conn.execute("DROP TABLE pending_summaries")
            self._conn.execute(
                "ALTER TABLE pending_summaries_new RENAME TO pending_summaries"
            )
            self._conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_queued_at
                ON pending_summaries(queued_at)
            """)
            self._conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_status
                ON pending_summaries(status)
            """)
            self._conn.commit()
            return  # Full recreate already has all columns

        # Incremental migration: add status/claimed columns
        if "status" not in columns:
            self._conn.execute(
                "ALTER TABLE pending_summaries ADD COLUMN status TEXT DEFAULT 'pending'"
            )
            self._conn.execute(
                "ALTER TABLE pending_summaries ADD COLUMN claimed_by TEXT"
            )
            self._conn.execute(
                "ALTER TABLE pending_summaries ADD COLUMN claimed_at TEXT"
            )
            self._conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_status
                ON pending_summaries(status)
            """)
            self._conn.commit()

        # Add error tracking and retry backoff columns
        if "last_error" not in columns:
            self._conn.execute(
                "ALTER TABLE pending_summaries ADD COLUMN last_error TEXT"
            )
            self._conn.execute(
                "ALTER TABLE pending_summaries ADD COLUMN retry_after TEXT"
            )
            self._conn.commit()

    def _recover_stale_claims(self) -> int:
        """Reset items claimed by crashed processors back to pending.

        Called at the start of dequeue. Items stuck in 'processing'
        longer than STALE_CLAIM_SECONDS are assumed to be from crashed
        processors and are reset.

        Returns count of recovered items.
        """
        cutoff = datetime.now(timezone.utc).isoformat()
        # Use a simple approach: check claimed_at timestamp
        cursor = self._conn.execute("""
            UPDATE pending_summaries
            SET status = 'pending', claimed_by = NULL, claimed_at = NULL
            WHERE status = 'processing'
              AND claimed_at IS NOT NULL
              AND julianday(?) - julianday(claimed_at) > ? / 86400.0
        """, (cutoff, STALE_CLAIM_SECONDS))
        recovered = cursor.rowcount
        if recovered:
            self._conn.commit()
            logger.info("Recovered %d stale claims from crashed processors", recovered)
        return recovered

    def enqueue(
        self,
        id: str,
        collection: str,
        content: str,
        *,
        task_type: str = "summarize",
        metadata: Optional[dict] = None,
    ) -> None:
        """
        Add an item to the pending queue.

        If the same (id, collection, task_type) already exists, replaces it
        (resets to pending status).
        Different task types for the same document coexist independently.
        """
        now = datetime.now(timezone.utc).isoformat()
        meta_json = json.dumps(metadata) if metadata else "{}"
        with self._lock:
            self._conn.execute("""
                INSERT OR REPLACE INTO pending_summaries
                (id, collection, content, queued_at, attempts, task_type, metadata,
                 status, claimed_by, claimed_at)
                VALUES (?, ?, ?, ?, 0, ?, ?, 'pending', NULL, NULL)
            """, (id, collection, content, now, task_type, meta_json))
            self._conn.commit()

    def dequeue(self, limit: int = 10) -> list[PendingSummary]:
        """
        Atomically claim the oldest pending items for processing.

        Uses BEGIN IMMEDIATE to get an exclusive write lock, then
        selects and claims items in a single transaction. Concurrent
        processors will block briefly, then see only unclaimed items.

        Items transition from 'pending' to 'processing'. Call complete()
        after success or fail() to return to pending.
        """
        pid = str(os.getpid())
        now = datetime.now(timezone.utc).isoformat()

        with self._lock:
            # Recover stale claims from crashed processors
            self._recover_stale_claims()

            # BEGIN IMMEDIATE acquires a write lock immediately,
            # preventing any other writer from interleaving
            self._conn.execute("BEGIN IMMEDIATE")
            try:
                # Select oldest pending items, respecting retry backoff
                cursor = self._conn.execute("""
                    SELECT id, collection, content, queued_at, attempts,
                           task_type, metadata
                    FROM pending_summaries
                    WHERE status = 'pending'
                      AND (retry_after IS NULL OR retry_after <= ?)
                    ORDER BY queued_at ASC
                    LIMIT ?
                """, (now, limit))

                rows = cursor.fetchall()
                items = []
                for row in rows:
                    meta = {}
                    if row[6]:
                        try:
                            meta = json.loads(row[6])
                        except (json.JSONDecodeError, TypeError):
                            pass
                    items.append(PendingSummary(
                        id=row[0],
                        collection=row[1],
                        content=row[2],
                        queued_at=row[3],
                        attempts=row[4],
                        task_type=row[5] or "summarize",
                        metadata=meta,
                    ))

                # Claim them atomically
                if items:
                    keys = [
                        (pid, now, item.id, item.collection, item.task_type)
                        for item in items
                    ]
                    self._conn.executemany("""
                        UPDATE pending_summaries
                        SET status = 'processing',
                            claimed_by = ?,
                            claimed_at = ?,
                            attempts = attempts + 1
                        WHERE id = ? AND collection = ? AND task_type = ?
                    """, keys)

                self._conn.commit()
            except Exception:
                self._conn.rollback()
                raise

        return items

    def complete(
        self, id: str, collection: str, task_type: str = "summarize"
    ) -> None:
        """Remove an item from the queue after successful processing."""
        with self._lock:
            self._conn.execute("""
                DELETE FROM pending_summaries
                WHERE id = ? AND collection = ? AND task_type = ?
            """, (id, collection, task_type))
            self._conn.commit()

    def fail(
        self, id: str, collection: str, task_type: str = "summarize",
        error: str | None = None,
    ) -> None:
        """Release a claimed item back to pending with exponential backoff.

        The attempt counter (already incremented by dequeue) is preserved.
        Retry delay: min(BASE * 2^(attempts-1), MAX) seconds.
        Error message is stored for diagnosis.
        """
        with self._lock:
            # Read current attempt count to compute backoff
            cursor = self._conn.execute(
                "SELECT attempts FROM pending_summaries "
                "WHERE id = ? AND collection = ? AND task_type = ?",
                (id, collection, task_type),
            )
            row = cursor.fetchone()
            attempts = row[0] if row else 1

            delay = min(RETRY_BACKOFF_BASE * (2 ** (attempts - 1)), RETRY_BACKOFF_MAX)
            now = datetime.now(timezone.utc)
            retry_at = (now + timedelta(seconds=delay)).isoformat()

            self._conn.execute("""
                UPDATE pending_summaries
                SET status = 'pending', claimed_by = NULL, claimed_at = NULL,
                    last_error = ?, retry_after = ?
                WHERE id = ? AND collection = ? AND task_type = ?
            """, (error, retry_at, id, collection, task_type))
            self._conn.commit()

            logger.info(
                "Item %s failed (attempt %d), retry after %ds: %s",
                id, attempts, delay, error or "unknown",
            )

    def abandon(
        self, id: str, collection: str, task_type: str = "summarize",
        error: str | None = None,
    ) -> None:
        """Move an item to 'failed' status (dead letter).

        Called when an item has exhausted all retry attempts. The item
        is preserved with its error message for diagnosis, not deleted.
        """
        with self._lock:
            self._conn.execute("""
                UPDATE pending_summaries
                SET status = 'failed', claimed_by = NULL, claimed_at = NULL,
                    last_error = ?
                WHERE id = ? AND collection = ? AND task_type = ?
            """, (error, id, collection, task_type))
            self._conn.commit()
            logger.warning("Abandoned %s/%s (%s): %s", collection, id, task_type, error or "max attempts")

    def count(self) -> int:
        """Get count of pending items (excludes processing and failed)."""
        cursor = self._conn.execute(
            "SELECT COUNT(*) FROM pending_summaries WHERE status = 'pending'"
        )
        return cursor.fetchone()[0]

    def stats(self) -> dict:
        """Get queue statistics including status breakdown."""
        cursor = self._conn.execute("""
            SELECT status, COUNT(*) as cnt
            FROM pending_summaries
            GROUP BY status
        """)
        by_status = {row[0] or "pending": row[1] for row in cursor.fetchall()}

        cursor = self._conn.execute("""
            SELECT
                COUNT(*) as total,
                COUNT(DISTINCT collection) as collections,
                MAX(attempts) as max_attempts,
                MIN(queued_at) as oldest
            FROM pending_summaries
        """)
        row = cursor.fetchone()
        return {
            "pending": by_status.get("pending", 0),
            "processing": by_status.get("processing", 0),
            "failed": by_status.get("failed", 0),
            "total": row[0],
            "collections": row[1],
            "max_attempts": row[2] or 0,
            "oldest": row[3],
            "queue_path": str(self._queue_path),
            "by_type": self.stats_by_type(),
        }

    def stats_by_type(self) -> dict[str, int]:
        """Get count of pending items grouped by task type."""
        cursor = self._conn.execute("""
            SELECT task_type, COUNT(*) as cnt
            FROM pending_summaries
            WHERE status IN ('pending', 'processing')
            GROUP BY task_type
            ORDER BY cnt DESC
        """)
        return {row[0]: row[1] for row in cursor.fetchall()}

    def list_failed(self) -> list[dict]:
        """List items in failed (dead letter) status.

        Returns list of dicts with id, collection, task_type, attempts,
        last_error, queued_at.
        """
        cursor = self._conn.execute("""
            SELECT id, collection, task_type, attempts, last_error, queued_at
            FROM pending_summaries
            WHERE status = 'failed'
            ORDER BY queued_at ASC
        """)
        return [
            {
                "id": row[0], "collection": row[1], "task_type": row[2],
                "attempts": row[3], "last_error": row[4], "queued_at": row[5],
            }
            for row in cursor.fetchall()
        ]

    def retry_failed(self) -> int:
        """Reset all failed items back to pending for retry.

        Resets attempt counters and clears backoff. Returns count of
        items moved back to pending.
        """
        with self._lock:
            cursor = self._conn.execute("""
                UPDATE pending_summaries
                SET status = 'pending', attempts = 0, claimed_by = NULL,
                    claimed_at = NULL, last_error = NULL, retry_after = NULL
                WHERE status = 'failed'
            """)
            self._conn.commit()
            count = cursor.rowcount
            if count:
                logger.info("Reset %d failed items back to pending", count)
            return count

    def get_status(self, id: str) -> dict | None:
        """Get pending task status for a specific note.

        Returns dict with id, task_type, status, queued_at if the note
        has pending work, or None if no work is pending.
        """
        with self._lock:
            cursor = self._conn.execute(
                "SELECT id, task_type, queued_at, status FROM pending_summaries WHERE id = ?",
                (id,),
            )
            row = cursor.fetchone()
        if row is None:
            return None
        return {
            "id": row[0],
            "task_type": row[1],
            "status": row[3] or "queued",
            "queued_at": row[2],
        }

    def clear(self) -> int:
        """Clear all pending items. Returns count of items cleared."""
        with self._lock:
            cursor = self._conn.execute("SELECT COUNT(*) FROM pending_summaries")
            count = cursor.fetchone()[0]
            self._conn.execute("DELETE FROM pending_summaries")
            self._conn.commit()
        return count

    def close(self) -> None:
        """Close the database connection."""
        if self._conn is not None:
            self._conn.close()
            self._conn = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
        return False

    def __del__(self):
        """Ensure connection is closed on garbage collection."""
        self.close()
